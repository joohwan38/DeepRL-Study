{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "planegame_RL.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "1u42PkN7GL-w5Th6yhSA58T9zC2CLnVcV",
      "authorship_tag": "ABX9TyO4deTQEVQ7LFiWaMwDPAxT",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/joohwan38/DeepRL-Study/blob/main/planegame_RL.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aK-5tkrHkvui",
        "outputId": "a9894341-fef4-497b-c161-f7aa8ecad302"
      },
      "source": [
        "!pip install stable_baselines3\n",
        "!pip install gym\n",
        "!pip install pygame"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: stable_baselines3 in /usr/local/lib/python3.7/dist-packages (1.1.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from stable_baselines3) (1.19.5)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from stable_baselines3) (1.1.5)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (from stable_baselines3) (3.2.2)\n",
            "Requirement already satisfied: torch>=1.4.0 in /usr/local/lib/python3.7/dist-packages (from stable_baselines3) (1.9.0+cu102)\n",
            "Requirement already satisfied: gym>=0.17 in /usr/local/lib/python3.7/dist-packages (from stable_baselines3) (0.17.3)\n",
            "Requirement already satisfied: cloudpickle in /usr/local/lib/python3.7/dist-packages (from stable_baselines3) (1.3.0)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from gym>=0.17->stable_baselines3) (1.4.1)\n",
            "Requirement already satisfied: pyglet<=1.5.0,>=1.4.0 in /usr/local/lib/python3.7/dist-packages (from gym>=0.17->stable_baselines3) (1.5.0)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from pyglet<=1.5.0,>=1.4.0->gym>=0.17->stable_baselines3) (0.16.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=1.4.0->stable_baselines3) (3.7.4.3)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->stable_baselines3) (2.4.7)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->stable_baselines3) (1.3.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib->stable_baselines3) (0.10.0)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->stable_baselines3) (2.8.2)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from cycler>=0.10->matplotlib->stable_baselines3) (1.15.0)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas->stable_baselines3) (2018.9)\n",
            "Requirement already satisfied: gym in /usr/local/lib/python3.7/dist-packages (0.17.3)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from gym) (1.4.1)\n",
            "Requirement already satisfied: pyglet<=1.5.0,>=1.4.0 in /usr/local/lib/python3.7/dist-packages (from gym) (1.5.0)\n",
            "Requirement already satisfied: numpy>=1.10.4 in /usr/local/lib/python3.7/dist-packages (from gym) (1.19.5)\n",
            "Requirement already satisfied: cloudpickle<1.7.0,>=1.2.0 in /usr/local/lib/python3.7/dist-packages (from gym) (1.3.0)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from pyglet<=1.5.0,>=1.4.0->gym) (0.16.0)\n",
            "Requirement already satisfied: pygame in /usr/local/lib/python3.7/dist-packages (2.0.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I6M3gmZukjXF",
        "outputId": "41c97431-ca66-4a66-f90e-ecfe68250c78"
      },
      "source": [
        "#import GYM stuff\n",
        "import gym \n",
        "from gym import Env\n",
        "from gym.spaces import Discrete, Box, Dict, Tuple, MultiBinary, MultiDiscrete \n",
        "\n",
        "#import helpers\n",
        "import numpy as np\n",
        "import random\n",
        "import os\n",
        "\n",
        "#import Stable baselines stuff\n",
        "from stable_baselines3 import PPO\n",
        "from stable_baselines3.common.vec_env import VecFrameStack\n",
        "from stable_baselines3.common.evaluation import evaluate_policy\n",
        "\n",
        "#import pygame\n",
        "import pygame\n",
        "import time\n",
        "from datetime import datetime\n",
        "\n",
        "# pygame.quit()"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "pygame 2.0.1 (SDL 2.0.14, Python 3.7.11)\n",
            "Hello from the pygame community. https://www.pygame.org/contribute.html\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g3sxSSzpkoWq"
      },
      "source": [
        "pygame.init()\n",
        "#오브젝트 좌표, 이속, 사이즈 초기값 세팅\n",
        "class obj:\n",
        "    def __init__(self):\n",
        "        self.x = 0\n",
        "        self.y = 0\n",
        "        self.move = 0\n",
        "    def put_img(self, a, b):\n",
        "        self.sx, self.sy = a, b\n",
        "\n",
        "#충돌 판정 (y/n)\n",
        "def crash(a, b):\n",
        "    if (a.x-b.sx <= b.x) and (b.x <= a.x+a.sx):\n",
        "        if (a.y-b.sy <= b.y) and (b.y <= a.y+a.sy):\n",
        "            return True\n",
        "        else:\n",
        "            return False\n",
        "    else:\n",
        "        return False\n",
        "\n",
        "#커스텀 환경 세팅)\n",
        "\n",
        "class PlaneEnv(Env):\n",
        "    \n",
        "    def __init__(self):\n",
        "        # Actions 조작의 범위 설정\n",
        "        self.action_space = Discrete(3) \n",
        "        # 옵저베이션 설정 (x축, y축)\n",
        "        self.observation_space = MultiDiscrete([500, 500, 900, 500, 900, 500, 900, 500, 900, 500, 900])\n",
        "        # 시작 값 설정 (all zero)\n",
        "        self.state = np.zeros(11, dtype=int)\n",
        "        \n",
        "        self.ss = obj()\n",
        "        #비행선 사이즈\n",
        "        self.ss.put_img(50, 60)\n",
        "        \n",
        "        #비행선 시작 좌표\n",
        "        self.ss.x = 205 #round(size[0]/2 - self.ss.sx/2)\n",
        "        self.ss.y = 825 #size[1] -self.ss.sy - 15\n",
        "        self.ss.move = 20\n",
        "        \n",
        "        \n",
        "# 1프레임마다 일어날 일돌 \n",
        "    def step(self, action):\n",
        "        global done\n",
        "        \n",
        "        #프레임당 생존 시 리워드 2점 지급\n",
        "        reward =2\n",
        "\n",
        "        #액션에 따른 비행선의 x좌표 변경 0 왼쪽 / 1 오른쪽 / 2 제자리 - 화면 가장자리를 넘어가지 않게 설정\n",
        "        if action ==0:\n",
        "            self.ss.x -= self.ss.move\n",
        "            if self.ss.x <= 50:\n",
        "                self.ss.x = 50\n",
        "        elif action ==1:\n",
        "            self.ss.x += self.ss.move\n",
        "            if self.ss.x >= 425:\n",
        "                self.ss.x = 425\n",
        "        elif action ==2:\n",
        "            pass\n",
        "\n",
        "        #랜덤하게 적 비행체 생성 (프레임당 10% 확률로 생성)\n",
        "        if random.random() > 0.90:\n",
        "            aa = obj()\n",
        "            aa.put_img(40, 40)\n",
        "\n",
        "            # 5배의 확률로 내가 멈춰있는 x좌표 위에 적군 생성 / 나머지는 전역 생성\n",
        "            mylist = [self.ss.x, random.randrange(0,478)]\n",
        "            aa.x = random.choices(mylist, weights = [10, 2], k = 1)[0]\n",
        "            aa.y = 10\n",
        "            aa.move = 25\n",
        "            a_list.append(aa)\n",
        "\n",
        "        #레이더 설정    \n",
        "        for i in range(len(a_list)):\n",
        "            #y 좌표가 나보다 낮아지면 레이더 계산하지 않음\n",
        "            if a_list[i].y < 825:\n",
        "                pass\n",
        "            #비행체의 중심부 - 적비행체 중심부간 유클리디안 거리가 100픽셀 보다 낮을때 레이더 발동\n",
        "            #레이더에 걸리면, 100 나누기 둘사이 거리의 10분의 1만큼 감점 수식 (100 / (distance / 10))\n",
        "            elif (abs(self.ss.x+25 - a_list[i].x+20) * abs(self.ss.y+30 - a_list[i].y+20))**(1/2) < 100:\n",
        "                reward -= round(100 / (((abs(self.ss.x+25 - a_list[i].x+20) * abs(self.ss.y+30 - a_list[i].y+20))**(1/2)) / 10))\n",
        "\n",
        "        #충돌 시 감점 로직    \n",
        "        for i in range(len(a_list)):\n",
        "            a = a_list[i]\n",
        "            if crash(a, self.ss) == True:\n",
        "                done=True\n",
        "                reward -=400\n",
        "        \n",
        "        # 현재 프레임의 옵저베이션값 리턴 (state)\n",
        "        self.state[0] = self.ss.x\n",
        "        for i in range(len(a_list[:(len(self.state)-1)//2])):\n",
        "            self.state[2*i+1] = a_list[i].x\n",
        "            self.state[2*i+2] = a_list[i].y\n",
        "\n",
        "        #미사일의 좌표를 화면을 넘어가면 트래킹 하지 않게 삭제해주기\n",
        "        \n",
        "        d_list = []\n",
        "        for i in range(len(a_list)):\n",
        "            ea = a_list[i]\n",
        "            ea.y += ea.move\n",
        "            if ea.y >= size[1]:\n",
        "                d_list.append(i)\n",
        "        #미사일이 화면 밖으로 넘어갔다는 것은 회피를 위미하므로, 회피시 리워드 지급하기\n",
        "                reward +=30\n",
        "        for d in d_list:\n",
        "            del a_list[d]\n",
        "        \n",
        "        info = {}\n",
        "        \n",
        "        # Return step information\n",
        "        return self.state, reward, done, info\n",
        "\n",
        "\n",
        "\n",
        "    def render(self):\n",
        "        pass\n",
        "    \n",
        "    #게임리셋 (환경 초기화, 비행체 시작위치 초기화)\n",
        "    #적 비행체는 초기화 하지 않아도 됨 (프레임에서 생성하기 때문에 환경초기화 만으로 전체초기화 됨)\n",
        "    def reset(self):\n",
        "        # Reset shower temperature\n",
        "        self.state = np.zeros(11, dtype=int)\n",
        "        # Set shower length\n",
        "        self.ss.x = 205 #round(size[0]/2 - self.ss.sx/2)\n",
        "        self.ss.y = 825 #size[1] -self.ss.sy - 15\n",
        "        \n",
        "        return self.state"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "doWurlgFlOiO"
      },
      "source": [
        "#환경 불러오기\n",
        "size = [500, 900]\n",
        "\n",
        "env=PlaneEnv()\n",
        "\n",
        "a_list=[]\n",
        "d_list=[]\n"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2dy8H9U-lR46",
        "outputId": "c9215f9a-2785-42fa-8560-92f94cae741b"
      },
      "source": [
        "#랜덤 액션 테스트 10회\n",
        "\n",
        "episodes = 10\n",
        "for episode in range(1, episodes+1):\n",
        "    state = env.reset()\n",
        "    done = False\n",
        "    score = 0\n",
        "    reward = 0\n",
        "    a_list=[]\n",
        "    d_list=[]\n",
        "    \n",
        "    while not done:\n",
        "        env.render()\n",
        "        action = env.action_space.sample()\n",
        "        n_state, reward, done, info = env.step(action)\n",
        "        score+=reward\n",
        "#         print(action, n_state, score, done)\n",
        "    print('Episode:{} Score:{}'.format(episode, score))\n",
        "env.close()"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Episode:1 Score:-328\n",
            "Episode:2 Score:-316\n",
            "Episode:3 Score:-324\n",
            "Episode:4 Score:-308\n",
            "Episode:5 Score:-318\n",
            "Episode:6 Score:-333\n",
            "Episode:7 Score:-324\n",
            "Episode:8 Score:-310\n",
            "Episode:9 Score:-278\n",
            "Episode:10 Score:-525\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sBj0rVM9lTlH"
      },
      "source": [
        "#모델 트레이닝\n",
        "log_path = os.path.join('Training', 'Logs')"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aGwb0znMlVs5",
        "outputId": "0fa89808-9231-4d61-8b8d-c408d21c38ec"
      },
      "source": [
        "model = PPO(\"MlpPolicy\", env, verbose=1, tensorboard_log=log_path)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using cpu device\n",
            "Wrapping the env with a `Monitor` wrapper\n",
            "Wrapping the env in a DummyVecEnv.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZYRW71yMlXHL",
        "outputId": "69845ac3-8a22-48b2-c825-32b7566a99a5"
      },
      "source": [
        "#콜백 하지 않기\n",
        "model.learn(total_timesteps=500000)#, callback=eval_callback)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Logging to Training/Logs/PPO_3\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 1        |\n",
            "|    ep_rew_mean     | -224     |\n",
            "| time/              |          |\n",
            "|    fps             | 537      |\n",
            "|    iterations      | 1        |\n",
            "|    time_elapsed    | 3        |\n",
            "|    total_timesteps | 2048     |\n",
            "---------------------------------\n",
            "-------------------------------------------\n",
            "| rollout/                |               |\n",
            "|    ep_len_mean          | 1             |\n",
            "|    ep_rew_mean          | -114          |\n",
            "| time/                   |               |\n",
            "|    fps                  | 314           |\n",
            "|    iterations           | 2             |\n",
            "|    time_elapsed         | 13            |\n",
            "|    total_timesteps      | 4096          |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 0.00014518062 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -1.1          |\n",
            "|    explained_variance   | 0             |\n",
            "|    learning_rate        | 0.0003        |\n",
            "|    loss                 | 5.1e+04       |\n",
            "|    n_updates            | 10            |\n",
            "|    policy_gradient_loss | -0.000173     |\n",
            "|    value_loss           | 1.16e+05      |\n",
            "-------------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 1            |\n",
            "|    ep_rew_mean          | -192         |\n",
            "| time/                   |              |\n",
            "|    fps                  | 276          |\n",
            "|    iterations           | 3            |\n",
            "|    time_elapsed         | 22           |\n",
            "|    total_timesteps      | 6144         |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 7.519181e-05 |\n",
            "|    clip_fraction        | 0            |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -1.1         |\n",
            "|    explained_variance   | -1.19e-07    |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 3.5e+04      |\n",
            "|    n_updates            | 20           |\n",
            "|    policy_gradient_loss | -7.53e-05    |\n",
            "|    value_loss           | 9.39e+04     |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 1            |\n",
            "|    ep_rew_mean          | -215         |\n",
            "| time/                   |              |\n",
            "|    fps                  | 259          |\n",
            "|    iterations           | 4            |\n",
            "|    time_elapsed         | 31           |\n",
            "|    total_timesteps      | 8192         |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0003399039 |\n",
            "|    clip_fraction        | 0            |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -1.1         |\n",
            "|    explained_variance   | 0            |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 5.83e+04     |\n",
            "|    n_updates            | 30           |\n",
            "|    policy_gradient_loss | -0.000407    |\n",
            "|    value_loss           | 9.03e+04     |\n",
            "------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C6P7LIVcvFa9"
      },
      "source": [
        "#모델 저장하기\n",
        "plane_path = os.path.join('Training', 'Saved Models', 'evolution_10M_PPO')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1a7eu8i6ldbU"
      },
      "source": [
        "model.save(plane_path)\n",
        "evaluate_policy(model, env, n_eval_episodes=100, render=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EwFq5Uwgl280"
      },
      "source": [
        "# training_log_path = os.path.join(log_path, 'PPO_20')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vqd-TPWepGOE"
      },
      "source": [
        "#모델 불러오기\n",
        "model = PPO.load(plane_path, env)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iP0AWJtWpRJJ"
      },
      "source": [
        "#어떤식으로 움직이고, 어떤 값을 리턴하는지 확인해보기 (랜더링 없이)\n",
        "\n",
        "s_list=[]\n",
        "\n",
        "episodes = 10\n",
        "for episode in range(1, episodes+1):\n",
        "    obs = env.reset()\n",
        "    done = False\n",
        "    score = 0\n",
        "    reward = 0\n",
        "    a_list=[]\n",
        "    d_list=[]\n",
        "    \n",
        "    k=0\n",
        "    while not done:\n",
        "        k+=1\n",
        "        env.render()\n",
        "        action, _states = model.predict(obs)\n",
        "        obs, reward, done, info = env.step(action)\n",
        "        score+=reward\n",
        "        if k %10 == 0:\n",
        "          print(obs, action, score)\n",
        "    s_list.append(k)\n",
        "    print('Episode:{} Score:{} Total Steps:{}'.format(episode, score, k))\n",
        "print('mean K:{}'.format(np.mean(s_list)))\n",
        "env.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W4gUy3E7pTiG"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RbDWkUIyyQzc"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}